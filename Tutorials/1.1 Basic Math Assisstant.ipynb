{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39f6bb9",
   "metadata": {},
   "source": [
    "This jupyter notebook demonstrates how to set up a simple customisable Chatbot that can solve basic maths questions. The first thing we need to do is install to the jupyter notebook we are using the openai module. You do this you use the line \"pip install openai\", once the cell has run once it does not need to be run again so you can put a # in front of it to make it text instead of a piece of code so you do not accidentally run it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba79c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca98d3",
   "metadata": {},
   "source": [
    "Then we import the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ac8a9",
   "metadata": {},
   "source": [
    "We can then define the openai api key as a string as any time we make reference to a function that communicates with OpenAI we need to include it so it knows it is us using it. I've included the one here that I have been using but if you distribute it futher you'll most likely want to have someone else use their own or use a different one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faac568",
   "metadata": {},
   "source": [
    "Next we define client within OpenAI using the api key string that we defined previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc22c1",
   "metadata": {},
   "source": [
    "We can then using the extension .beta.assistants.create on the client to create the first assistant. We'll give it several properties to start off with. Name is how it is refered to as wtihin the OpenAI website. Instructions are the very basic prompts we give it as to how it should behave. We have given it a single tool here which is \"code_interpreter\" this allows the the chatbot to execute python code which allows it to perform data analysis, automate tasks and solve mathematical problems. The model decides which model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Math Tutor\",\n",
    "  instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "  tools=[{\"type\": \"code_interpreter\"}],\n",
    "  model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900ddf8",
   "metadata": {},
   "source": [
    "Next we create a thread, a thread is a conversation between you and one or many assistants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e531dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12423976",
   "metadata": {},
   "source": [
    "Next we attach a message object to the thread, they can contain both texts and files and there is no limit on th number of messages able to be added to a file. Here thread_id decides the thread on which the message is added to. Role decides where the message is coming from, in this case the user is asking the assistant a question but we could have as easily had the message been from the system or the assistant. In the case of the system sending a message this would be essentialy identical to the instructions we gave to the assistant when creating the assistant. Usually in the case of the message being from the assistant these will be generated by the chatbot but can be manually entered as well. The content of the message is the string that the message contains and is what will actually be sent to be interpreted by the chatbot in this case. Feel free to try swapping out the content of the message to another simple maths question, something like \"I need a basic y=x graph plot. Could you make it for me?\" will show you the code for the plotted graph in python that the assistant makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6a37d",
   "metadata": {},
   "source": [
    "Next we define a class known as the EventHandler, it essentually decides on how the text from the chatbot is shown to us. The first import known as \"override\" is a decorator used to indicate a method is overriding a method in the superclass. The second import is the \"AssistantEventHandler\" which is the base class for handling events in OpenAI.\n",
    "\n",
    "The class EventHandler has 4 functions defined inside of it which are used to define custom behaviours for different events in respons to the response stream that we get from the chat bot. The first function is the \"on_text_created\" which simply prints \"assistant >\" before the assistant responds so we know it is the assistant responding. The \"on_text_delta\" function is called dependent on a delta.value which gives a time dependence to the generation of text so that each piece of generated text by the assistant is given incramentally and provides real time feedback. \"on_tool_call_created\" is called when a tool is generated, in our case the code interpreter is our tool and so when it is used the code will print the type of tool initiated by the chatbot. The final function is \"on_tool_call_delta\" this adds like previous delta function incramental updates to the tool call such that when the code_interpreter tool is used it prints the code being sent to the code interpreter and provides detailed feedback on the codes execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "  \n",
    "class EventHandler(AssistantEventHandler):    \n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "  \n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\noutput >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e87162",
   "metadata": {},
   "source": [
    "Finally we retrieve the stream of responses for a specific thread, once again we define the thread_id that we have used and we do the same for the assistant_id, we can also provide some more simple instructions and finally we pass our event handler class into event_handler so the text comes out the way we want it from the chat bot. The stream.until_done() function gives our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Alex.\",\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
