{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b81156",
   "metadata": {},
   "source": [
    "If you try experimenting with asking one of the chat bots we have created previously about its vector store you may have found it hard to get answers about just something from that vector store, as such we need a way of narrowing the chat bots focus on a specific sets of information. To do this we can use what is known as an RAG (retrieval augmented generation) pipeline. What this essentially does is it splits the source of information into several parts known as \"chunks\" these chunks are then \"embedded\" which means they are turned into large vectors and stored (I briefly touched on this in the file supported jupyter notebook). We then embed the users prompt and compare each chunk's vector embed against the users prompt vector embed and see how similar they are. We then feed the most similar chunks back to the chat bot and ask it to answer the prompt in light of this information. We start as we usually do and start off by defining our input query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how do i find the minimum chi squared value?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142406a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai \n",
    "import numpy as np\n",
    "\n",
    "openai_api_key = \"\"\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce516ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Discovery Skills\",\n",
    "  description=\"You are a factual education AI Assistant dedicated to providing accurate, useful information. Your primary task is to assist me by providing me reliable and clear responses to my questions, only ever use information from file search as your source, this knowledge base is ______.  You are reluctant of making any claims unless they are stated or supported by the knowledge base.\",\n",
    "  instructions = \"if you make any code you should always run it\",\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}],\n",
    "  top_p=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737177db",
   "metadata": {},
   "source": [
    "For this to work we'll need to locate the folder the reference document is inside of, assuming that your reference document is inside the same folder as this jupyter notebook you can run the below box and replace the file_path string with the output of the box and then re run the box. If your reference document is in another location simply take the below and enter it in a jupyter notebook in the same folder as the document and then come back to this one and swap the file_path out for the output you recieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2813d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the above file path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd292955",
   "metadata": {},
   "source": [
    "This is our chunking method, we first define our chunks list and then define a load_large_document function. This function opens our reference document as a file, it then reads the file up to a number of characters equal to our chunk_size and then yields that as a string. We use yield instead of return as it allows for the use of a for loop to be much easier in the next line of code. This for loop uses the load_large_document function to break the document into 2000 character long chunks which are then added into our chunks list. We can then print the chunks list to see the entire document. \n",
    "\n",
    "Using a lower chunk size increases the amount of time taken for this process to complete however it increases the final specificity of the answer, you do not want to go too low however as this would lead to there not being enough information at the end to provide the user with a useful answer. A bigger chunk size allows for this process to be quicker but decreases specificity in the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "\n",
    "def load_large_document(file_path, chunk_size=2000):\n",
    "    with open(file_path, 'r') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "\n",
    "for chunk in load_large_document('file_name.txt'):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ea058",
   "metadata": {},
   "source": [
    "We then call .embeddings.create at our client to embed the chunks we have just created, we specify a model to be used (this is the most general use case one but there are others), we specify our input to be embedded which in this case is our list of chunks, and our encouding format deicdes how the vectors appear, the alternate encoding formate is base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.embeddings.create(\n",
    "  model=\"text-embedding-ada-002\",\n",
    "  input=chunks,\n",
    "  encoding_format=\"float\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20129c85",
   "metadata": {},
   "source": [
    "We'll do the same thing to embed our query except this time we'll also flatten the embedding such that our query is a singular vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = client.embeddings.create(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    input=[query],\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "query_embedding = query_response.data[0].embedding\n",
    "query_embedding = np.array(query_embedding).flatten()\n",
    "\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533a380",
   "metadata": {},
   "source": [
    "Now we'll compare the similarity of our query's embeddding with every single embedding of our chunks. We'll do this using defintion of the angle between two vectors, the closer the output of our cosine is to 1 the closer in relavence a given chunk and our query is, we refer to this number as a similarity score and we'll create a list of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "similarity_scores = []\n",
    "for embedding_data in db.data:\n",
    "    chunk_embedding = embedding_data.embedding\n",
    "\n",
    "    chunk_embedding = np.array(chunk_embedding).flatten()\n",
    "\n",
    "    score = cosine_similarity(query_embedding, chunk_embedding)\n",
    "    similarity_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ae8b8",
   "metadata": {},
   "source": [
    "Next we'll combine our chunk list and similarity score list and sort the list in order of highest similarity to lowest, we'll then take the top 5 chunks in similarity to our query and combine them into a single string. By printing it we can see what chunks have been chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37009f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_scores = list(zip(chunks, similarity_scores))\n",
    "sorted_chunks = sorted(chunk_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_n = 5\n",
    "combined_string = \"All documents sourced from: file_name.txt\"\n",
    "for i in range(top_n):\n",
    "    combined_string += sorted_chunks[i][0]\n",
    "    \n",
    "print(combined_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa5899",
   "metadata": {},
   "source": [
    "Now that we have both a refined source material and our query we can combine these into a single prompt and give it to the chat bot. We will use the same event handler as we did in the code_interpreter jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2694a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Answer the question based only on this context: {combined_string}. Answer the question based on the above context: {query}, you should explain as if i do not have access to this context, always quote _____ as your source. run any code you create, quote explicitely any equations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt,        \n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "\n",
    "class EventHandler(AssistantEventHandler):    \n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "  \n",
    "    def on_tool_call_delta(self, delta, snapshot):\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "                    elif output.type == \"image\":\n",
    "                        # Fetch the image data using the file_id\n",
    "                        file_id = output.image.file_id\n",
    "                        image_data = self.download_image(file_id)\n",
    "                        if image_data:\n",
    "                            image = Image.open(io.BytesIO(image_data))\n",
    "                            image.show()\n",
    "  \n",
    "    def download_image(self, file_id):\n",
    "        url = f\"https://api.openai.com/v1/files/{file_id}/content\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Failed to download image: {response.status_code} {response.text}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"\",\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7434294",
   "metadata": {},
   "source": [
    "Because the chat bot has also has access to the entire textbook it can still answer questions about where the content comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"what chapter could i find more in about this stuff?\"\n",
    ")\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"\",\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
