{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25b1719",
   "metadata": {},
   "source": [
    "Now we'll create a chatbot that is able to take information from files and answer questions about them. As we did with the basic math chat bot we first install openai, import it, define the api key, define the client object and finally define the assistant. This time we're using the \"file_search\" tool and I have used the 2nd year F2A quantum notes for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf68a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your OpenAI API key\n",
    "openai_api_key = \"\"\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    " \n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Quantum teacher\",\n",
    "  instructions=\"You are an expert Quantum physicist. Use you knowledge base to answer questions about quantum physics.\",\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27524d5",
   "metadata": {},
   "source": [
    "Next we need to create a vector store. Vector stores are essentially massive vectors which files can be turned into which the AI is then able to interpret. \n",
    "\n",
    "(Side tangent that is not totally nessecary for understanding this code but is useful to know: the way chatbots work is through the comparison of these vectors. Lets say for example you took the word \"amazon\" and on a scale of 0 to 1 you compared it to the word \"beach\" where 0 is as close as possible and 1 is as far away as possible you might get a value of something like 0.3. You may then compare it to the word \"jungle\" and get 0.1 but you could also compare it to the word \"shopping\" and get 0.07. The point is the space that contains all these vectors makes important of the semantics of words and its through this comparison that AI is able to generate answers. Although not explicitly stated in the following code what the AI is doing is it is breaking up the source document into several hundred \"chunks\" all of equal character length. It then compares the vectors of the chunks against the vectors of our instructions and promts and picks the most similar chunks. Then it feeds these chunks into the chatbot which it then bases its answers on.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"Quantum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922ee81",
   "metadata": {},
   "source": [
    "We then prep all the files that we want to upload to OpenAI by stating the file paths, in this case because the \"quantumnotes.txt\" is a sibling to the jupyter notebook file we can simple state the file name. Make note of the fact that file_paths is a list and so several files could be included usually however these should be .txt, .pdf or .md. file_streams is a python list of file objects opened in binary read mode, essentially it just makes it so we can turn it into a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"quantumnotes.txt\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102abdb1",
   "metadata": {},
   "source": [
    "We then use the an extenstion of the client to upload the file_streams, we make sure to define the vector store that we want to upload the files to and the files that we want to upload. We then also print the status and counts of the files as they upload so that we can ensure that everything has gone through properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247846a",
   "metadata": {},
   "source": [
    "As we did in the simple maths chatbot we define an assistant with the relavent id and tool resources this time making sure to give it access to the vector store we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc63330",
   "metadata": {},
   "source": [
    "Here we can also upload the file seperately to OpenAI so that its id can be used in a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58942933",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_file = client.files.create(\n",
    "  file=open(\"quantumnotes.txt\", \"rb\"), purpose=\"assistants\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04533ff8",
   "metadata": {},
   "source": [
    "Again as we did previously we create a thread this time for speed we can include the message inside the thread, the only thing being different is the \"attachments\" which takes the id of the message_file so it knows which vector store to use and the tools so it knows to use file_search. Again we can print a log of this happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b669c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"what is degenerate perturbation theory\",\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c457a1c",
   "metadata": {},
   "source": [
    "Again as we did previously we define an event handler this time there is only one new function which is \"on_message_done\" simply provides a citation to all of the files used. In our case there is only one file so several citations may lead back to the same file but in the case of uploading several files to the vector store, this would provide citations to each of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d32445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    " \n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e083651",
   "metadata": {},
   "source": [
    "Once again we do as we did before inputing the relavent ids and such. Also excuse the horribly formatted mathematics it makes, I will try to improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Please address the user as Alex.\",\n",
    "    event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9307105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
