{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112169b1",
   "metadata": {},
   "source": [
    "This jupyter notebook demonstrates the use of a fine tuned model designed to help students towards answers by asking questions rather than just giving answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai\n",
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how would i make a curve fitting program?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142406a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "openai_api_key = \"\"\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4f8e9",
   "metadata": {},
   "source": [
    "Here we've created an assistant as ususal however in this we have specified a custom fine tuned model. To create a fine tuned model you can follow these steps: Go to the OpenAI dashboard, on the left hand menu go to fine-tuning, in the top right click the +create button to begin customising a fine-tuning job, select your base model (if its text based i'd recommend one of the gpt models), upload your training data (I will go over this more in a second), provide a suffix (this is essentially a name for the model), then I'd recommend leaving everything else as is originally (set to auto/random) but if you find the fine tuned model to be unresponsive to your training data you can up the learning multiplier to fit the model more to your data (increasing this too much can lead to overfitting however, for this example I used a value of 2). You can retrieve the name of the model from the same place as you created it under the \"Output Model\" Section. This should then be put in the model section when creating the assistant as done below.\n",
    "\n",
    "Regarding training data, in the same folder as you find this file you should find a .jsonl file, you can open this with notepad (or a text editor of your choice) and inside it you will find a large list of messages formatted like: \n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a factual chatbot that helps students learn by asking them questions\"}, {\"role\": \"user\", \"content\": \"What is the area of a rectangle with a length of 5 and a width of 3?\"}, {\"role\": \"assistant\", \"content\": \"How do you calculate the area of a rectangle?\"}, {\"role\": \"user\", \"content\": \"You multiply the length by the width.\"}, {\"role\": \"assistant\", \"content\": \"That's correct. So, what is 5 multiplied by 3?\"},{\"role\": \"user\", \"content\": \"5 multiplied by 3 is 15.\"}, {\"role\": \"assistant\", \"content\": \"Great! So, what is the area of the rectangle?\"},{\"role\": \"user\", \"content\": \"The area is 15.\"}, {\"role\": \"assistant\", \"content\": \"Exactly. Well done!\"}]}, \n",
    "\n",
    "each of these lines that start with {\"messages\":.... and are example conversations between an assistant and a user. After the \"messages\": we specify a list of messages. The first message in the list is always a system message, we define roles by using \"role\": and then give the role of the entity giving the message as we have done previously when creating messages in threads. Similarly as we have done previusly after specifying the role we specify the content of the message given by that role, in this case the first system message is \"You are a factual chatbot that helps students learn by asking them questions\". The next messages all follow the same format but contain different content and use the user and assistant roles, this is the example conversation between the user and assistant. To make all of this more obvious to you I've reformatted the above to give the below:\n",
    "\n",
    "{\n",
    "\"messages\": \n",
    "\n",
    "[\n",
    "\n",
    "{\"role\": \"system\", \"content\": \"You are a factual chatbot that helps students learn by asking them questions\"}, \n",
    "\n",
    "{\"role\": \"user\", \"content\": \"What is the area of a rectangle with a length of 5 and a width of 3?\"},\n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"How do you calculate the area of a rectangle?\"}, \n",
    "\n",
    "{\"role\": \"user\", \"content\": \"You multiply the length by the width.\"}, \n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"That's correct. So, what is 5 multiplied by 3?\"},\n",
    "\n",
    "{\"role\": \"user\", \"content\": \"5 multiplied by 3 is 15.\"}, \n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"Great! So, what is the area of the rectangle?\"},\n",
    "\n",
    "{\"role\": \"user\", \"content\": \"The area is 15.\"},\n",
    "\n",
    "{\"role\": \"assistant\", \"content\": \"Exactly. Well done!\"}\n",
    "\n",
    "]\n",
    "\n",
    "}, \n",
    "\n",
    "Several of these can then be uploaded to OpenAI in a .jsonl file to serve as training data. OpenAI recommends 50-100 messages to begin getting behaviour that you want but more is always good. Remember we're not training the knowledge we want here, we're training the type of conversation we want. \n",
    "\n",
    "It is important in our case that all of the first system messages in each conversation is the same. When we create an assistant based on this model we need to give it instructions that are identical to the initial system message and we need to make sure these instructions are never overwritten. When we give the model the identical instructions we are essentially telling it that we want it to mimic the behaviour it has seen in the .json conversations. Therefore any instructions you want to give the model should be specified in the .json file initially. If you want to give more instructions on the fly, I'd recommend putting these in the prompt text or in a description however the latter is limited to 512 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce516ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Coding Teacher\",\n",
    "  instructions=\"You are a factual chatbot that helps students learn by asking them questions\",\n",
    "  model=\"ft:gpt-3.5-turbo-0125:personal:socratic4:9eLlcKvZ\",\n",
    "  tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}],\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [\"vs_jqaQHX3axAiwMZDTWATBHdQL\"]}},\n",
    "  top_p=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517fe48",
   "metadata": {},
   "source": [
    "I'll also make note here that I have created a new vector store without code, instead as we did above for fine-tuning I created it in the OpenAI dashboard. To do this follow these steps: Go to the OpenAI dashboard, on the left hand menu go to storage, then switch to the Vector Stores tab, here you'll want to press the +create button in the top right (if there are any pre exisiting vector stores make sure you switch to the new one you just created before the next step as it does not auto select this new vector store for you), once you've done this you can press the +add files button to add files to the vector store. A list of the type of files that a vector store accepts can be found here: https://platform.openai.com/docs/assistants/tools/file-search/supported-files. Once we have created a vector store and uploaded our files we can retrieve the vector store id from the same window next to where it says \"ID\". Above I have manually put this in to our tool reasources, in this example I'm using a series of .py files from L1 that teach students how to code (due to privacy reasons this couldnt be shared on GitHub, please use a repository of knowledge that you find useful for your usecase). Below I've specified this as a string that I can use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"Year 1 Python Handbook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2d868",
   "metadata": {},
   "source": [
    "Change file_path to be wherever the folder for where the knowledge base files are locally stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2813d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40510c",
   "metadata": {},
   "source": [
    "As well as creating the vector store with all the files, if we want to use the custom search that we created it needs an update to be able to handle more file types and more files. To handle several files we'll define the get_all_files_in_folder which retrieves the file paths of all the files in that folder and returns them as a list. We can print this to make sure it's getting all of them. Then in the load_large_document to save ourselves some headache in the future we can update it to be able to extract text from pdfs (in another later jupyter notebook we go over how to extract images). We keep the original load_large_document part intact. A similar approach can be done for different file types if they are not able to be extracted by our original load_large_document in the same way as we have done for pdfs. We then define a function called process_files which calls the load_large_document function for each of the files found by get_all_files_in_folder and splits the text from them into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_in_folder(folder_path):\n",
    "    try:\n",
    "        # List all entries in the directory\n",
    "        entries = os.listdir(folder_path)\n",
    "        \n",
    "        # Filter out the files and include the full path\n",
    "        files = [os.path.join(folder_path, entry) for entry in entries if os.path.isfile(os.path.join(folder_path, entry))]\n",
    "        \n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        return \"The folder path does not exist.\"\n",
    "    \n",
    "files = get_all_files_in_folder(file_path)\n",
    "\n",
    "print(files)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "def load_large_document(file_path, chunk_size=2000):\n",
    "    if file_path.endswith('.pdf'):\n",
    "        try:\n",
    "            reader = PdfReader(file_path)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "                \n",
    "            for i in range(0, len(text), chunk_size):\n",
    "                yield text[i:i + chunk_size]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                while True:\n",
    "                    chunk = file.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    yield chunk\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading the file {file_path}: {e}\")\n",
    "\n",
    "def process_files(file_paths, chunk_size=2000):\n",
    "    for file_path in file_paths:\n",
    "        for chunk in load_large_document(file_path, chunk_size):\n",
    "            chunks.append(chunk)\n",
    "\n",
    "process_files(files)\n",
    "\n",
    "print(chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40294cf9",
   "metadata": {},
   "source": [
    "Our embedding and similarity checks are the same as previous, just a note for future optimisations, although the query embedding needs to be created everytime there is a new query the embedding for the chunks does not, these can be stored and retrieved. To make the final product more efficient you can create the embedding on the initial upload of the knowledge base documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.embeddings.create(\n",
    "  model=\"text-embedding-ada-002\",\n",
    "  input=chunks,\n",
    "  encoding_format=\"float\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = client.embeddings.create(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    input=[query],\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "query_embedding = query_response.data[0].embedding\n",
    "query_embedding = np.array(query_embedding).flatten()\n",
    "\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "similarity_scores = []\n",
    "for embedding_data in db.data:\n",
    "    chunk_embedding = embedding_data.embedding\n",
    "\n",
    "    chunk_embedding = np.array(chunk_embedding).flatten()\n",
    "\n",
    "    score = cosine_similarity(query_embedding, chunk_embedding)\n",
    "    similarity_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37009f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_scores = list(zip(chunks, similarity_scores))\n",
    "sorted_chunks = sorted(chunk_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_n = 5\n",
    "combined_string = f\"All documents sourced from: {source}\"\n",
    "for i in range(top_n):\n",
    "    combined_string += sorted_chunks[i][0]\n",
    "    \n",
    "print(combined_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159c875",
   "metadata": {},
   "source": [
    "We've updated the prompt a little to be more in line with the fine tune model we created specifying that it should use the \"socratic method\" which is a summed up version of saying dont give answers only ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2694a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Answer the question based only on this context: {combined_string}. Answer the question based on the above context: {query}, you should explain as if i do not have access to this context and any source documents, this is your source always quote it when you use it: {source}, run any code you create, quote explicitely any equations in latex format, always use $ when writing latex, employ the socratic method for teaching in your answer, speak as if you're talking with a student.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt,\n",
    "    },\n",
    "    \n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "from PIL import Image\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "import io\n",
    "import requests\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.buffer = \"\"  # Buffer to collect text output\n",
    "        \n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        if not self.buffer.endswith(text.value):  # Prevent duplication\n",
    "            self.buffer += text.value\n",
    "\n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        if not self.buffer.endswith(delta.value):  # Prevent duplication\n",
    "            self.buffer += delta.value\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "            \n",
    "    def on_tool_call_delta(self, delta, snapshot):\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True)\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True)\n",
    "                    elif output.type == \"image\":\n",
    "                        # Fetch the image data using the file_id\n",
    "                        file_id = output.image.file_id\n",
    "                        image_data = self.download_image(file_id)\n",
    "                        if image_data:\n",
    "                            image = Image.open(io.BytesIO(image_data))\n",
    "                            image.show()\n",
    "  \n",
    "    def download_image(self, file_id):\n",
    "        url = f\"https://api.openai.com/v1/files/{file_id}/content\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Failed to download image: {response.status_code} {response.text}\")\n",
    "            return None\n",
    "        \n",
    "    def display_output(self):\n",
    "\n",
    "        # Process the buffer to format LaTeX and code blocks\n",
    "        processed_content = self.format_buffer(self.buffer)\n",
    "\n",
    "        # Display as Markdown to correctly render LaTeX and plain text\n",
    "        display(Markdown(processed_content))\n",
    "\n",
    "    def format_buffer(self, buffer):\n",
    "        # Format the buffer to handle LaTeX and code blocks appropriately\n",
    "\n",
    "        # Split buffer into lines for processing\n",
    "        lines = buffer.split(\"\\n\")\n",
    "        formatted_lines = []\n",
    "\n",
    "        in_code_block = False\n",
    "\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"code_interpreter\"):\n",
    "                in_code_block = True\n",
    "                formatted_lines.append(line)\n",
    "            elif line.strip() == \"```\":\n",
    "                in_code_block = False\n",
    "                formatted_lines.append(line)\n",
    "            elif in_code_block:\n",
    "                formatted_lines.append(line)\n",
    "            else:\n",
    "                # Check for LaTeX patterns and wrap them in delimiters\n",
    "                line = line.replace(r'\\(', '$').replace(r'\\)', '$')\n",
    "                line = line.replace(r'\\[', '$$').replace(r'\\]', '$$')\n",
    "                formatted_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(formatted_lines)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4509d4",
   "metadata": {},
   "source": [
    "Here we can see that the chatbot should ask us questions to help us towards the answer rather than giving it straight away "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_handler = EventHandler()\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  event_handler=event_handler,\n",
    ") as stream:\n",
    "  stream.until_done()\n",
    "\n",
    "event_handler.display_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1c3e2",
   "metadata": {},
   "source": [
    "As usual we can continue the conversation using the below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"im not sure\"\n",
    ")\n",
    "\n",
    "event_handler2 = EventHandler()\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"\",\n",
    "  event_handler=event_handler2,\n",
    ") as stream:\n",
    "  stream.until_done()\n",
    "\n",
    "event_handler2.display_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81da5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
