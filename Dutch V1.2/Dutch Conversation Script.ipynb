{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook goes over the process of creating a starting a conversation with the chatbot, please make sure to use the \"Resource Creator\" and the \"Assistant Creator\" before you run this script.\n",
    "\n",
    "In this notebook we retrieve all the files we created in the previous notebook and use them to create a conversation with the chatbot.\n",
    "\n",
    "# Contents\n",
    "\n",
    "**1 - Setup**\n",
    "\n",
    "- 1.1 Imports\n",
    "- 1.2 OpenAI\n",
    "- 1.3 Directories\n",
    "- 1.4 Previous Files\n",
    "\n",
    "**2 - Knowledge Retrieval Functions**\n",
    "\n",
    "- 2.1 Query Embedding and Comparison\n",
    "- 2.2 Chunking and Specified Knowledge Retrieval\n",
    "- 2.3 Image Retrieval\n",
    "\n",
    "**3 - Conversation**\n",
    "\n",
    "- 3.1 Event Handler\n",
    "- 3.2 Conversation Handler\n",
    "- 3.3 Pre-Conversation Functions\n",
    "- 3.4 Conversation UI\n",
    "- 3.5 Previous Conversation Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup\n",
    "\n",
    "This section details all of the basic peices we need to put together for our chatbot to function, it does not include the functions we use for the chatbot but does detail the imports, OpenAI functionality, directories and the loading of the files we set up in the \"Assistant Creator\" and \"Resources Creator\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "\n",
    "These are our imports for this notebook, I'll go over what each are used for now:\n",
    "\n",
    "`from openai import OpenAI` the OpenAI module allows us to set up a client that can communicate with OpenAI's services. These services are not specific to just chatbots although it does include this purpose we can use these services to create vector stores (more on this later) and upload and change files.\n",
    "\n",
    "`import os` this module allows us to modify and access files and folders\n",
    "\n",
    "`import json` this module allows for the reading and creation of .json files which allow us to store the data we process for later use\n",
    "\n",
    "`import requests` this module allows us to make external requests to outside urls, specifically we will be making requests to OpenAI\n",
    "\n",
    "`from PIL import Image` this module allows for the storage and retreival of images given image data\n",
    "\n",
    "`import pickl` this module allows us to store what cannot be in json files due to information being non-subscriptable\n",
    "\n",
    "`import numpy as np` this module is for math process'\n",
    "\n",
    "`import datetime` allows us to access the current date and time for giving conversation files names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any errors are returned when trying to run the above due to modules not being installed you can remove the # from the appropriate commands below to install the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai\n",
    "#pip install os\n",
    "#pip install json\n",
    "#pip install requests\n",
    "#pip install PIL\n",
    "#pip install pickle\n",
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 OpenAI\n",
    "\n",
    "We define the defnitions needed for OpenAI so we can easily access them later\n",
    "\n",
    "`api_key =` this is essentially a password provided by OpenAI, it allows us to access OpenAI's services whenever we use them\n",
    "\n",
    "`client = OpenAI(api_key=api_key)` this sets up a client which can communicate with OpenAI's services, we specify this beforehand so we do not have to write out \"OpenAI(api_key=api_key)\" when we want to communicate with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Directories\n",
    "\n",
    "We set up any directories for files or websites that we will use later\n",
    "\n",
    "`store_name =` this is a general purpose name that we will use when creating files, this allows us to make sure we are retrieving the documents we want later on.\n",
    "\n",
    "`assistant_directory =` this is the file directory where we store and retrieve the assistant id from. \n",
    "\n",
    "`document_directory =` this is the file directory where we store and retrieve our documents from.\n",
    "\n",
    "`data_directory =` this is the file directory where we'll store and retrieve any other kinds of data.\n",
    "\n",
    "`image_directory =` this is the file directory where we'll store and retreieve any images.\n",
    "\n",
    "`conversation_directory =` this is the directory we'll store conversations we have with the chatbot as .json files so we can look back at them later.\n",
    "\n",
    "`urls =` you should specify here any urls you want the assistant to have access to\n",
    "\n",
    "`urls_with_subdomains =` this list specifies any urls with subdomains all of which you want to add to the urls list\n",
    "\n",
    "You should make sure when specifying these that they are the same as you used in the Assistant Creator and Resource Creator Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name = \"Programming\"\n",
    "\n",
    "assistant_directory = r\"\"\n",
    "\n",
    "document_directory = r\"\"\n",
    "\n",
    "data_directory = r\"\"\n",
    "\n",
    "image_directory = r\"\"\n",
    "\n",
    "conversation_directory = r\"\"\n",
    "\n",
    "urls = []\n",
    "\n",
    "urls_with_subdomains = [\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Previous Files\n",
    "\n",
    "We need retrieve all of the items we stored, the very last one of these you'll need to input the \"assistant_name\" manually as when the assistants are created you chose their name specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allias_name = f\"allias_{store_name}.json\" \n",
    "allias_path = os.path.join(data_directory, allias_name) # gets the path for our allias'\n",
    "\n",
    "with open(allias_path, \"r\") as file:\n",
    "    allias = json.load(file) # retrieves our allias'\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "image_names_list = f\"{store_name}_image_names.json\"\n",
    "image_names_path = os.path.join(data_directory, image_names_list) # gets the path for our image names\n",
    "\n",
    "with open(image_names_path, \"r\") as file:\n",
    "    image_names = json.load(file) # retrieves the image names\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "descriptions_name = f\"{store_name}_descriptions.json\" \n",
    "descriptions_path = os.path.join(data_directory, descriptions_name) # gets the path for our descriptions\n",
    "\n",
    "with open(descriptions_path, \"r\") as file:\n",
    "    descriptions = json.load(file) # retrieves our descriptions\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "chunks_name = f\"chunks_{store_name}.json\"\n",
    "chunks_path = os.path.join(data_directory, chunks_name) # gets the paths for our chunks\n",
    "\n",
    "with open(chunks_path, \"r\") as file:\n",
    "    chunks = json.load(file) # retrieves our chunks\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "data_path = os.path.join(data_directory, f\"{store_name}database.pkl\") # gets the path for the embedding file\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    db = pickle.load(f) # retrieves our embedding\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "vector_name = f\"vector_store_id_{store_name}.json\"\n",
    "vector_path = os.path.join(data_directory, vector_name) # gets the path for our vector store id\n",
    "\n",
    "with open(vector_path, \"r\") as file: \n",
    "    vector_store_id = json.load(file) # retrieves our vector store id\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "assistant_name = f\"Programming Teacher_assistant_id.json\" # YOU NEED TO CHANGE THIS TO THE ONE YOU CREATED!!!!\n",
    "assistant_path = os.path.join(assistant_directory, assistant_name) # gets the path for our vector store id\n",
    "\n",
    "with open(assistant_path, \"r\") as file: \n",
    "    assistant_id = json.load(file) # retrieves our vector store id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Knowledge Retrieval Functions\n",
    "\n",
    "This section covers the basic functions our chatbot will need to ensure it is retrieving the knowledge most relevant to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Query Embedding and Comparison\n",
    "\n",
    "We start off by embedding our querry as we did for the chunks previously, this allows us to compare the similarity of the query to each of the different chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedd(query):    \n",
    "    query_response = client.embeddings.create( #creates an embedding\n",
    "        model=\"text-embedding-ada-002\", # picks a model to use for embedding, this is a general purpose one for text but there are others for other purposes\n",
    "        input=[query], # selects what list we want to use for our embedding\n",
    "        encoding_format=\"float\" # selects what format the embedding is in, the other option is base64\n",
    "    )\n",
    "\n",
    "    query_embedding1 = query_response.data[0].embedding \n",
    "    query_embedding2 = np.array(query_embedding1).flatten() # we turn our query embedding into a single flattened vector \n",
    "    return query_embedding2 # we return the flattened embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that compares the similarity of two vectors using the angle between them as a measure, our two vectors will be the embedding of the query and each of the different chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)) # formula for the cosine of an angle between two vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function that takes the embedding of a chunk and uses the cosine_similarity function to compare it against the embedding of our query. We store all of the output values in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(query_embedding):\n",
    "    similarity_scores = [] # creates a list of similarity scores\n",
    "    for embedding_data in db.data: # loops through the data assisgned to each chunk\n",
    "        chunk_embedding = embedding_data.embedding # retrieves the embedding from a specific chunks embedding\n",
    "\n",
    "        chunk_embedding = np.array(chunk_embedding).flatten() # turns our embedding into a single flattened vector\n",
    "\n",
    "        score = cosine_similarity(query_embedding, chunk_embedding) # compares the similarity of the two vectors\n",
    "        similarity_scores.append(score) # store the similarity of the two vectors\n",
    "    return similarity_scores # returns the list of similarity scores between the query and the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Chunking and Specified Knowledge Retrieval\n",
    "\n",
    "We now define a series of functions that allow us to retrieve the most relevant chunks to our given query\n",
    "\n",
    "This is function retrieves a list of all the files in a given folder path. We'll use these folder paths as a way of referencing which document the chunks we end up using come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_in_folder(folder_path):\n",
    "    try:\n",
    "        entries = os.listdir(folder_path) # Makes a list of all entries in a given directory\n",
    "        \n",
    "        files = [os.path.join(folder_path, entry) for entry in entries if os.path.isfile(os.path.join(folder_path, entry))] # combines the entries with the folder directory so that we have their full file path\n",
    "        \n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        return \"The folder path does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function that takes the similarity scores and finds the top chunks that are most similar to the user query and outputs these top chunks into a single string, it checks the start of each of the chunks for where they are sourced from and adds these sources to a list of sources. It uses the allias' defined in the reasource creator so that when we give the sources to the user they are more readable. If no allias are given then the source is simply the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_string(similarity_scores):\n",
    "    files = get_all_files_in_folder(document_directory)\n",
    "    chunk_scores = list(zip(chunks, similarity_scores)) # combines the chunks and similarity scores into a list with each chunk indexed against its similarity score\n",
    "    sorted_chunks = sorted(chunk_scores, key=lambda x: x[1], reverse=True) # sorts the list according to the similarity scores\n",
    "\n",
    "    top_n = 5 # decides how many chunks to be accepted into the string\n",
    "    combined_string = f\"\"\n",
    "\n",
    "    all_sources =[] # empty string for sources of each chunk\n",
    "    sources = [] # empty string for non duplicate sources\n",
    "\n",
    "    for i in range(top_n):\n",
    "        combined_string += sorted_chunks[i][0] # combines the top chunks into a single string\n",
    "\n",
    "        for file in files:  # Loop through all the file names\n",
    "            base_filename = os.path.basename(file)\n",
    "            chunk_prefix = sorted_chunks[i][0]\n",
    "            \n",
    "            if chunk_prefix.startswith(base_filename):  # Check the chunk to see which document it is sourced from\n",
    "                found = False\n",
    "                for n in range(len(allias)):\n",
    "                    if base_filename == allias[n][0]:\n",
    "                        all_sources.append(allias[n][1])  # Add the source to a list\n",
    "                        found = True\n",
    "                        break  # Exit the inner 'for n' loop\n",
    "                if not found:  # This runs only if no alias match was found\n",
    "                    all_sources.append(base_filename) # adds the file name as the source\n",
    "                break  # Exit the outer 'for file' loop to avoid unnecessary iterations\n",
    "\n",
    "    sources = list(set(all_sources)) # removes duplicates from the list\n",
    "    \n",
    "    return combined_string, sources # returns our string and the sources of each of the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval\n",
    "\n",
    "Given a user query we also want to retrieve an image most relevant to the user query to do this we create a prompt to send to gpt-4o: \n",
    "`out of this list {descriptions} which relates most to {query}, if none of them relate to the query state only the word false otherwise state only a single number starting with the first being 0 as this is being used for indexing in python`,\n",
    " this prompt asks gpt-4o which out of the descriptions of the images which are most relevant to the query and if none are it returns `false`. We can then use this output to decide which image to show to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_retrieval(query):  \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    prompt = f\"Out of this list {descriptions} which relates most to {query}, if none of them relate to the query state only the word false otherwise state only a single number starting with the first being 0 as this is being used for indexing in python, please do not be afraid to use the word false, you should only return indexes on average 30% of the time.\"\n",
    "    message = {  # this is our \"message\" it's what we're actually sending to OpenAI\n",
    "        \"model\": \"gpt-4o\",  # model to send the prompt to\n",
    "        \"messages\": [  # contains the main content of what we want to send\n",
    "            {\n",
    "                \"role\": \"user\",  # specifies that this is a message from a user\n",
    "                \"content\": [  # what is included in our message, what gpt-4o will see\n",
    "                    {\n",
    "                        \"type\": \"text\",  # specifies what the following input is\n",
    "                        \"text\": prompt  # we tell gpt-4o what we want it to do\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300  # sets a limit on the number of tokens to be used per message, tokens equate to processing which equates to money, so by limiting this we keep cost and processing time down\n",
    "        # note that increases in the max_tokens is necessary for more complex tasks\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=message)  # we post the message to chat/completions\n",
    "    response_data = response.json()\n",
    "    index = response_data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    if \"a\" in index or \"A\" in index: # checks if there is a relavent image\n",
    "        pass\n",
    "    else:\n",
    "        index = int(index)\n",
    "        image_name = image_names[index]\n",
    "        image_path = os.path.join(image_directory, image_name)\n",
    "        description = descriptions[index]\n",
    "        return Image.open(image_path), description, image_path, str(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Conversation\n",
    "\n",
    "This section details the Event Handler and functions that are run when we have a conversation with the chatbot. These functions contain most if not all of the functions we have defined previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Event Handler\n",
    "\n",
    "This is our event handler class, it essentially handles how our output looks, it handles what we as a user see. This event handler is able to retrieve images that our chatbot creates such as graphs it generates through its code_interpreter and it displays any text the chatbot writes in a markdown box which provides a readable form, it also makes it so that the text is updated as the chatbot writes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output # imports that handle the display\n",
    "import requests # allows us to retrieve images from OpenAI\n",
    "from PIL import Image # allows us to display those images\n",
    "import io # allows us convert images to the relevant format\n",
    "from openai import AssistantEventHandler # allows us to create an event handler\n",
    "from typing_extensions import override # allows us to use override\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        super().__init__()\n",
    "        self.buffer = \"\"  # Buffer to collect text output\n",
    "        self.sources = sources  # List to collect sources\n",
    "        self.final_output = \"\"  # Variable to store final output\n",
    "\n",
    "    @override # @override is a decorator used to explicitly indicate that this method is overriding a method in the superclass\n",
    "    def on_text_created(self, text):\n",
    "        self.update_buffer(text.value) # stores and updates the buffer text with the initial text\n",
    "\n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        self.update_buffer(delta.value) # stores and updates the buffer text with new text as the chatbot writes \n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True) # tells us when the chatbot uses a tool \n",
    "            \n",
    "    def on_tool_call_delta(self, delta, snapshot): # updates as a tool is used, specifically for the code interpreter to show images that it creates\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True) # tells us that the code interpreter is giving an output\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True) # prints the logs of the code interpreter\n",
    "                    elif output.type == \"image\":\n",
    "                        file_id = output.image.file_id\n",
    "                        image_data = self.download_image(file_id) # downloads the image from OpenAI using the download image function\n",
    "                        if image_data:\n",
    "                            image = Image.open(io.BytesIO(image_data))\n",
    "                            image.show() # shows the downloaded image to the user\n",
    "\n",
    "    def download_image(self, file_id):\n",
    "        url = f\"https://api.openai.com/v1/files/{file_id}/content\" # the url for where assistant generated outputs go\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\", # our api key in our header\n",
    "        }\n",
    "        response = requests.get(url, headers=headers) # gets the response from OpenAI\n",
    "        if response.status_code == 200:\n",
    "            return response.content # returns the image\n",
    "        else:\n",
    "            print(f\"Failed to download image: {response.status_code} {response.text}\")\n",
    "            return None\n",
    "        \n",
    "    def update_buffer(self, text): # this displays the output text and updates it as the chatbot writes more of the output\n",
    "        if not self.buffer.endswith(text):  # Prevent duplication\n",
    "            self.buffer += text\n",
    "            self.display_output() # displays the output\n",
    "\n",
    "    def display_output(self):\n",
    "        clear_output(wait=True) # Clear previous output\n",
    "        processed_content = self.format_buffer(self.buffer) # Process the buffer to format LaTeX and code blocks\n",
    "        processed_content += self.format_sources()  # Append sources information\n",
    "        self.final_output = processed_content  # Store final output\n",
    "        display(Markdown(processed_content)) # Display as Markdown to correctly render LaTeX and plain text\n",
    "\n",
    "    def format_buffer(self, buffer):\n",
    "       # Split buffer into lines for processing\n",
    "        lines = buffer.split(\"\\n\")\n",
    "        formatted_lines = []\n",
    "\n",
    "        in_code_block = False\n",
    "\n",
    "        for line in lines:\n",
    "            # checks to see if the lines are a code block\n",
    "            if line.strip().startswith(\"code_interpreter\"):\n",
    "                in_code_block = True\n",
    "                formatted_lines.append(line)\n",
    "            elif line.strip() == \"```\":\n",
    "                in_code_block = False\n",
    "                formatted_lines.append(line)\n",
    "            elif in_code_block:\n",
    "                formatted_lines.append(line)\n",
    "            else:\n",
    "                # Check for LaTeX patterns and wrap them in delimiters\n",
    "                line = line.replace(r'\\(', '$').replace(r'\\)', '$')\n",
    "                line = line.replace(r'\\[', '$$').replace(r'\\]', '$$')\n",
    "                formatted_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(formatted_lines) # joins all of the lines together\n",
    "\n",
    "    def format_sources(self):\n",
    "        if self.sources:\n",
    "            return f\"\\n\\nAll information has been sourced from {', '.join(self.sources)}\" #adds this line onto the end of all outputs so that sources are always given\n",
    "        return \"\"\n",
    "\n",
    "    def get_final_output(self):\n",
    "        return self.final_output  # Return the final processed content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Conversation Handler\n",
    "\n",
    "Our Conversation Handler consists of 2 functions the `start_conversation` function and the `continue_conversation` function, both work in very similar ways. \n",
    "\n",
    "**`start_conversation` function**\n",
    "This function takes in a string as its only argument, it passes this through the `query_embedd` function the result of which is passed through the `get_similarities` function the result of which is passed through the `get_combined_string` function which outputs the section of the knowledge base we want the assistant to focus on (Note: I say \"focus on\" here as the assistant has access to the full knowledge base as we set it up as such in the assistant creator). We then run the `image_retrieval` function to see if there are any images in the ones we have stored relevant to our user query. This next step is a process specific for a programming teaching chatbot but there is no reason you couldn't apply the same methodology elsewhere. To ensure that the assistant behaves the way we want it to, not outputting the final answer in full but instead helping the student step by step through the process, using the `combined_string` we have a regular model (gpt-4o) design a step by step process that answers the user query, we limit how much it writes in this step by specifying the token usage as this reduces cost and processing times. The next step is to then give our asssistant this step by step process, this acts almost like a markscheme and if we specify in the assistant prompt to only go through the \"markscheme\" one step at a time we end up with our desired behaviour.The reason we use gpt-4o instead of the assistant for the step by step process is for two reasons: 1 any file searches of the entire knowledge base that need to be performed can be performed by the assistant when we pass the query and markscheme to it and therefore do not need to be performed twice and if we pass it to the assistant we would be unable to use any fine tuning as our behaviour would not work properly. Before we pass the steps to the assistant we first check however if an image has been sucessfully retrieved, if it has we start a thread with a message that includes the combined string, the user query, the step by step process and the description of the image and we upload the image to open ai, if not we exclude the description of the image and do not upload any images to openai. In both scenarios we give the assistant instructions on how to deal with user queries and how to format its output. This function outputs the thread and the final output and if an image was given to the user the file information that can help us identify the images location on open ai and the local image path. We also append onto the final output the thread id and the image description.\n",
    "\n",
    "**`continue_conversation` function**\n",
    "This function is extremely similar to the `start_conversation` function aside from a few minor differences, this function takes in both a string and a thread as its arguments, the string is our user query and the thread is the identification so the conversation can continue from the previous message. We enact the same process for the step by step creation so the assistant stays on track and we send a message in the thread instead of creating a new one. This part does not in anyway currently enable the retrieval of images however questions about the original image can still be asked.\n",
    "\n",
    "Both of these functions use the `display_output` function from the eventhandler to display their outputs in a fashion that updates as the messages are written, they also call the `get_final_output` function from the eventhandler as this allows us to keep a log of all the messages written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(query): \n",
    "\n",
    "  query_embedding = query_embedd(query=query) # creates the query embedding\n",
    "  similarity_scores = get_similarities(query_embedding=query_embedding) # compares the query to the chunks\n",
    "  combined_string, sources = get_combined_string(similarity_scores=similarity_scores) # gets the best chunks releveant to the query\n",
    "  image_data = image_retrieval(query=query)\n",
    "  \n",
    "\n",
    "  headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "\n",
    "  prompt1 = f\"Based on this context {combined_string}, create a step by step answer to this {query}, your answer should include every detail someone may want to know and explain every step of the process.\"\n",
    "\n",
    "  message = { # this is our \"message\" it's what we're actually sending to OpenAI\n",
    "                \"model\": \"gpt-4o\", # model to send the prompt to \n",
    "                \"messages\": [ # contains the main content of what we want to send\n",
    "                    {\n",
    "                        \"role\": \"user\", # specifies that this is a message from a user\n",
    "                        \"content\": [ # what is included in our message, what gpt-4o will see\n",
    "                            {\n",
    "                                \"type\": \"text\", # specifies what the following input is\n",
    "                                \"text\": prompt1 # we tell gpt-4o what we want it to do\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"max_tokens\": 300 # sets a limit on the number of tokens to be used per message, tokens equate to processing which equates to money, so by limiting this we keep cost and processing time down\n",
    "            # note that increases in the max_tokens is nessecary for more complex tasks\n",
    "            }\n",
    "\n",
    "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=message)\n",
    "\n",
    "  if image_data:\n",
    "    image, description, image_path, image_name = image_data\n",
    "    prompt2 = f\"This is the user query: {query}, here is the answer to the user query in a step by step format {response}, respond to the user by providing hints as to what the next step in the process is rather than just outright giving them the answer, you should ask the user a question at the end of every single step that could help them get the next step, you should only go one step at a time per answer never ever more than this, you may give the user hints but again never give them the full answer, this is also some extra context you may need: {combined_string}. You should explain as if i do not have access to this context and any source documents. run any code you create. quote explicitely any equations in latex format. always use $ when writing latex, speak as if you're talking with a student, when you respond to a user question about a step, do not make a new step, instead answer the question they asked then repeat the step they asked the question about. Please be relatively conversational, so you do not need to title questions and hints as such. You should, but not for every step, ask if the user can explain some part of the previous step back to you before you move on to the next step or alternatively ask the user to write code for the next step before you show them the answer but again dont do this for every step. Sometimes ask the user to write code before you write it for them. Explain every single part of every line of any code. You do not need to title the steps. You have been provided an image, here is its description {description}, only mention the image if the user enquires about it. Never Ever give the user the answer in full in a single message. Please write no more than 300 words per message.\"\n",
    "    # the prompt to send to the user\n",
    "    file = client.files.create(file=open(image_path, \"rb\"), purpose=\"vision\")\n",
    "    thread = client.beta.threads.create(  # we create a thread (conversation)\n",
    "        messages=[  # what we want to send to the assistant\n",
    "            {\n",
    "                \"role\": \"user\",  # who we are sending the message from\n",
    "                \"content\": \n",
    "                [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt2,  # what our message is\n",
    "                    },\n",
    "                    {\n",
    "                    \"type\": \"image_file\",\n",
    "                    \"image_file\": {\"file_id\": file.id},\n",
    "                    },\n",
    "                    \n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    event_handler = EventHandler(sources=sources)  # we define the event handler which we want to use\n",
    "\n",
    "    with client.beta.threads.runs.stream(  # we send our thread to an assistant and specify what event handler to use\n",
    "        thread_id=thread.id,  # what thread we want to use\n",
    "        assistant_id=assistant_id,  # what assistant we want to use\n",
    "        event_handler=event_handler,  # what event handler we want to use\n",
    "    ) as stream:\n",
    "        stream.until_done()  # streams our output\n",
    "\n",
    "    event_handler.display_output()  # displays our output\n",
    "\n",
    "    \n",
    "    output = f\"{event_handler.get_final_output()}\"\n",
    "\n",
    "    image, description, image_path, image_name = image_data\n",
    "    image.show()  # an image is shown if there is a relevant image \n",
    "    output += f\"\\n\\n *Image Description of {image_name}*: {description}\" # append the description of the image to the text\n",
    "\n",
    "    output += f\"\\n\\n *Thread ID*: {thread.id}\"\n",
    "    return thread, output, file, image_path\n",
    "\n",
    "  else:\n",
    "     prompt2 = f\"This is the user query: {query}, here is the answer to the user query in a step by step format {response}, respond to the user by providing hints as to what the next step in the process is rather than just outright giving them the answer, you should ask the user a question at the end of every single step that could help them get the next step, you should only go one step at a time per answer never ever more than this, you may give the user hints but again never give them the full answer, this is also some extra context you may need: {combined_string}. You should explain as if i do not have access to this context and any source documents. run any code you create. quote explicitely any equations in latex format. always use $ when writing latex, speak as if you're talking with a student, when you respond to a user question about a step, do not make a new step, instead answer the question they asked then repeat the step they asked the question about. Please be relatively conversational, so you do not need to title questions and hints as such. You should, but not for every step, ask if the user can explain some part of the previous step back to you before you move on to the next step or alternatively ask the user to write code for the next step before you show them the answer but again dont do this for every step. Sometimes ask the user to write code before you write it for them. Explain every single part of every line of any code. You do not need to title the steps. Never Ever give the user the answer in full in a single message. Please write no more than 300 words per message\"\n",
    "    # the prompt to send to the user\n",
    "     thread = client.beta.threads.create(  # we create a thread (conversation)\n",
    "        messages=[  # what we want to send to the assistant\n",
    "            {\n",
    "                \"role\": \"user\",  # who we are sending the message from\n",
    "                \"content\": prompt2,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "     event_handler = EventHandler(sources=sources)  # we define the event handler which we want to use\n",
    "\n",
    "     with client.beta.threads.runs.stream(  # we send our thread to an assistant and specify what event handler to use\n",
    "        thread_id=thread.id,  # what thread we want to use\n",
    "        assistant_id=assistant_id,  # what assistant we want to use\n",
    "        event_handler=event_handler,  # what event handler we want to use\n",
    "    ) as stream:\n",
    "        stream.until_done()  # streams our output\n",
    "\n",
    "     event_handler.display_output()  # displays our output\n",
    "\n",
    "    \n",
    "     output = f\"{event_handler.get_final_output()}\"\n",
    "\n",
    "     output += f\"\\n\\n *Thread ID*: {thread.id}\"\n",
    "     return thread, output, None, None\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_conversation(query, thread,):\n",
    "\n",
    "  query_embedding = query_embedd(query=query)\n",
    "  similarity_scores = get_similarities(query_embedding=query_embedding)\n",
    "  combined_string, sources = get_combined_string(similarity_scores=similarity_scores)\n",
    "\n",
    "  headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {api_key}\"\n",
    "        }\n",
    "\n",
    "  prompt1 = f\"Based on this context {combined_string}, create a step by step answer to this {query}, your answer should include every detail someone may want to know and explain every step of the process.\"\n",
    "\n",
    "  message = { # this is our \"message\" it's what we're actually sending to OpenAI\n",
    "                \"model\": \"gpt-4o\", # model to send the prompt to \n",
    "                \"messages\": [ # contains the main content of what we want to send\n",
    "                    {\n",
    "                        \"role\": \"user\", # specifies that this is a message from a user\n",
    "                        \"content\": [ # what is included in our message, what gpt-4o will see\n",
    "                            {\n",
    "                                \"type\": \"text\", # specifies what the following input is\n",
    "                                \"text\": prompt1 # we tell gpt-4o what we want it to do\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"max_tokens\": 300 # sets a limit on the number of tokens to be used per message, tokens equate to processing which equates to money, so by limiting this we keep cost and processing time down\n",
    "            # note that increases in the max_tokens is nessecary for more complex tasks\n",
    "            }\n",
    "\n",
    "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=message)\n",
    "\n",
    "  prompt2 = f\"This is the user query: {query}, here is the answer to the user query in a step by step format {response}, respond to the user by providing hints as to what the next step in the process is rather than just outright giving them the answer, you should ask the user a question at the end of every single step that could help them get the next step, you should only go one step at a time per answer never ever more than this, you may give the user hints but again never give them the full answer, this is also some extra context you may need: {combined_string}. You should explain as if i do not have access to this context and any source documents. run any code you create. quote explicitely any equations in latex format. always use $ when writing latex, speak as if you're talking with a student, when you respond to a user question about a step, do not make a new step, instead answer the question they asked then repeat the step they asked the question about. Please be relatively conversational, so you do not need to title questions and hints as such. You should, but not for every step, ask if the user can explain some part of the previous step back to you before you move on to the next step or alternatively ask the user to write code for the next step before you show them the answer but again dont do this for every step. Sometimes ask the user to write code before you write it for them. Explain every single part of every line of any code. You do not need to title the steps. Never Ever give the user the answer in full in a single message. Please write no more than 300 words per message\"\n",
    "# the prompt to send to the user\n",
    "  thread_message = client.beta.threads.messages.create(\n",
    "      thread_id=thread.id,\n",
    "      role=\"user\",\n",
    "      content=prompt2\n",
    "  )\n",
    "\n",
    "  event_handler2 = EventHandler(sources=sources)\n",
    "\n",
    "  with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant_id,\n",
    "    instructions=\"\",\n",
    "    event_handler=event_handler2,\n",
    "  ) as stream:\n",
    "    stream.until_done()\n",
    "\n",
    "  event_handler2.display_output()\n",
    "\n",
    "  return event_handler2.get_final_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pre-Conversation Functions\n",
    "\n",
    "We also need to define two \"Pre-Conversation\" functions that allow the final interaction to be more efficient. \n",
    "\n",
    "The first is the `delete_image` function, this will be called whenever the user quits out of a conversation and deletes the image file that we uploaded to OpenAI in the conversation, this saves on storage space on the OpenAI server\n",
    "\n",
    "The second is the `save_conversation` function which will be called whenever the user quits out of a conversation and stores the conversation had locally and in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_image(file):\n",
    "    if file != None:\n",
    "        API_BASE_URL = f\"https://api.openai.com/v1\" # url where files are stored\n",
    "        headers = headers = {\n",
    "            'Authorization': f'Bearer {api_key}', # our password\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.delete(f\"{API_BASE_URL}/files/{file.id}\", headers=headers) # delete the file based on its id\n",
    "        print(f\"{file.id}:\", response.status_code, response.text) # tells us if it was sucessful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversation(conversation, image_path):\n",
    "    conversation_name = f\"Conversation_{datetime.datetime.now().strftime('%d-%m-%Y_%H-%M')}.json\" # specifies a name for the file based on date and time of conversation\n",
    "    conversation_path = os.path.join(conversation_directory, conversation_name) # specifies where to store the file\n",
    "    \n",
    "    if image_path != None:\n",
    "        conversation = f\"{conversation} \\n\\nImage Used:{image_path}\" # checks if an image was produced in the conversation and if it was appends the conversation to include that detail\n",
    "\n",
    "    with open(conversation_path, \"w\") as file:\n",
    "        json.dump(conversation, file) # saves the conversation\n",
    "    print(f\"conversation saved as {conversation_path}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Conversation UI\n",
    "\n",
    "We then use all of our previous functions to create the below box which allows us to have a conversation with our chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Enter your query:\") # takes an input\n",
    "messages = f\"**User**: {user_input} \\n\"\n",
    "display(Markdown(messages)) \n",
    "thread, assistant_output, file, image_path = start_conversation(user_input) # calls the start conversation function, giving an output and storing a thread\n",
    "clear_output(wait=True)\n",
    "messages  += f\"\\n **Assistant**: {assistant_output}\"\n",
    "display(Markdown(messages)) \n",
    "while user_input != \"quit\" and user_input != \"Quit\": # we use a while loop so that the conversation continues until the user enters \"quit\" or \"Quit\"\n",
    "    user_input = input() # takes another user input\\\n",
    "    clear_output(wait=True)\n",
    "    messages += f\"\\n\\n**User**: {user_input} \\n\"\n",
    "    if user_input == \"quit\" or user_input == \"Quit\": # checks if the user input is quit\n",
    "        messages += \"\\n\\nQuit Successful\"\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(messages))\n",
    "        delete_image(file)\n",
    "        save_conversation(messages, image_path)\n",
    "    else:  \n",
    "        assistant_output = continue_conversation(query=user_input, thread=thread,) # if the input was not quit we continue the conversation in the same thread\n",
    "        messages += f\"\\n **Assistant**: {assistant_output}\"\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(messages))       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Conversation Retrieval\n",
    "\n",
    "By specifying a file name in the `loaded_conversation_name =` section and running the 2nd below box we can load a previous conversation and view it's contents. The first box displays all the stored conversations for ease of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = get_all_files_in_folder(conversation_directory)\n",
    "\n",
    "for conversation in conversations:\n",
    "    print(os.path.basename(conversation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_conversation_name = f\"L4 Q11.json\" \n",
    "loaded_conversation_path = os.path.join(conversation_directory, loaded_conversation_name) # gets the path for our descriptions\n",
    "\n",
    "with open(loaded_conversation_path, \"r\") as file:\n",
    "    previous_conversation = json.load(file) # retrieves our descriptions\n",
    "\n",
    "display(Markdown(previous_conversation))\n",
    "\n",
    "if \"Image Used:\" in previous_conversation:\n",
    "    image_obtained = previous_conversation.split(\"Image Used:\")[-1].strip()\n",
    "    Image.open(image_obtained).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
