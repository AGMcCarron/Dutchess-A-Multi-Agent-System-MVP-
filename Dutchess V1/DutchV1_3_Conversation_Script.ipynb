{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook goes over the process of creating a starting a conversation with the chatbot, please make sure to use the \"Resource Creator\" and the \"Assistant Creator\" before you run this script.\n",
    "\n",
    "In this notebook we retrieve all the files we created in the previous notebook and use them to create a conversation with the chatbot.\n",
    "\n",
    "# Contents\n",
    "\n",
    "**1 - Setup**\n",
    "\n",
    "- 1.1 Imports\n",
    "- 1.2 OpenAI\n",
    "- 1.3 Directories\n",
    "- 1.4 Previous Files\n",
    "\n",
    "**2 - Knowledge Retrieval Functions**\n",
    "\n",
    "- 2.1 Query Embedding and Comparison\n",
    "- 2.2 Chunking and Specified Knowledge Retrieval\n",
    "- 2.3 Image Retrieval\n",
    "\n",
    "**3 - Conversation**\n",
    "\n",
    "- 3.1 Event Handler\n",
    "- 3.2 Conversation Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup\n",
    "\n",
    "This section details all of the basic peices we need to put together for our chatbot to function, it does not include the functions we use for the chatbot but does detail the imports, OpenAI functionality, directories and the loading of the files we set up in the \"Assistant Creator\" and \"Resources Creator\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "\n",
    "These are our imports for this notebook, I'll go over what each are used for now:\n",
    "\n",
    "`from openai import OpenAI` the OpenAI module allows us to set up a client that can communicate with OpenAI's services. These services are not specific to just chatbots although it does include this purpose we can use these services to create vector stores (more on this later) and upload and change files.\n",
    "\n",
    "`import os` this module allows us to modify and access files and folders\n",
    "\n",
    "`import json` this module allows for the reading and creation of .json files which allow us to store the data we process for later use\n",
    "\n",
    "`import requests` this module allows us to make external requests to outside urls, specifically we will be making requests to OpenAI\n",
    "\n",
    "`from PIL import Image` this module allows for the storage and retreival of images given image data\n",
    "\n",
    "`import pickle` this module allows us to store what cannot be in json files due to information being non-subscriptable\n",
    "\n",
    "`import numpy as np` this module is for math process'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any errors are returned when trying to run the above due to modules not being installed you can remove the # from the appropriate commands below to install the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai\n",
    "#pip install os\n",
    "#pip install json\n",
    "#pip install requests\n",
    "#pip install PIL\n",
    "#pip install pickle\n",
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 OpenAI\n",
    "\n",
    "We define the defnitions needed for OpenAI so we can easily access them later\n",
    "\n",
    "`api_key =` this is essentially a password provided by OpenAI, it allows us to access OpenAI's services whenever we use them\n",
    "\n",
    "`client = OpenAI(api_key=api_key)` this sets up a client which can communicate with OpenAI's services, we specify this beforehand so we do not have to write out \"OpenAI(api_key=api_key)\" when we want to communicate with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Directories\n",
    "\n",
    "We set up any directories for files that we will use later:\n",
    "\n",
    "`store_name =` this is a general purpose name that we will use when creating files, this allows us to make sure we are retrieving the documents we want later on.\n",
    "\n",
    "`data_directory =` this is the file directory where we'll store and retrieve any our data.\n",
    "\n",
    "`document_directory =` this is the file directory where we'll store and retrieve our documents from.\n",
    "\n",
    "`image_directory =` this is the file directory where we'll store and retreieve any images.\n",
    "\n",
    "`assistant_directory =` this is the file directory where we store and retrieve the assistant ids from. \n",
    "\n",
    "You should make sure when specifying these that they are the same as you used in the Assistant Creator and Resource Creator Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name = \"Labs Dutchess\"\n",
    "\n",
    "this_directory = os.getcwd()\n",
    "\n",
    "directories = os.listdir(this_directory)\n",
    "\n",
    "directories = [os.path.join(this_directory, entry) for entry in directories if not os.path.isfile(os.path.join(this_directory, entry))] \n",
    "\n",
    "for directory in directories:\n",
    "    if \"Data Base\" in os.path.basename(directory):\n",
    "        data_directory = directory\n",
    "    elif \"Documents\" in os.path.basename(directory):\n",
    "        document_directory = directory\n",
    "    elif \"Output Images\" in os.path.basename(directory):\n",
    "        image_directory = directory\n",
    "    elif \"Assistants\" in os.path.basename(directory):\n",
    "        assistant_directory = directory\n",
    "\n",
    "print(f\"data_directory = {data_directory}\")\n",
    "print(f\"document_directory = {document_directory}\")\n",
    "print(f\"image_directory = {image_directory}\")\n",
    "print(f\"assistant_directory = {assistant_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Previous Files\n",
    "\n",
    "We need retrieve all of the items we stored, for the very last one of these make sure you've changed the \"assistant_name\" manually in the box above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allias_name = f\"allias_{store_name}.json\" \n",
    "allias_path = os.path.join(data_directory, allias_name) # gets the path for our allias'\n",
    "\n",
    "with open(allias_path, \"r\") as file:\n",
    "    allias = json.load(file) # retrieves our allias'\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "image_names_list = f\"{store_name}_image_names.json\"\n",
    "image_names_path = os.path.join(data_directory, image_names_list) # gets the path for our image names\n",
    "\n",
    "with open(image_names_path, \"r\") as file:\n",
    "    image_names = json.load(file) # retrieves the image names\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "descriptions_name = f\"{store_name}_descriptions.json\" \n",
    "descriptions_path = os.path.join(data_directory, descriptions_name) # gets the path for our descriptions\n",
    "\n",
    "with open(descriptions_path, \"r\") as file:\n",
    "    descriptions = json.load(file) # retrieves our descriptions\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "chunks_name = f\"chunks_{store_name}.json\"\n",
    "chunks_path = os.path.join(data_directory, chunks_name) # gets the paths for our chunks\n",
    "\n",
    "with open(chunks_path, \"r\") as file:\n",
    "    chunks = json.load(file) # retrieves our chunks\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "data_path = os.path.join(data_directory, f\"{store_name}database.pkl\") # gets the path for the embedding file\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    db = pickle.load(f) # retrieves our embedding\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "if os.listdir(document_directory) != []:\n",
    "    vector_name = f\"vector_store_id_{store_name}.json\"\n",
    "    vector_path = os.path.join(data_directory, vector_name) # gets the path for our vector store id\n",
    "\n",
    "    with open(vector_path, \"r\") as file: \n",
    "        vector_store_id = json.load(file) # retrieves our vector store id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Knowledge Retrieval Functions\n",
    "\n",
    "This section covers the basic functions our chatbot will need to ensure it is retrieving the knowledge most relevant to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Query Embedding and Comparison\n",
    "\n",
    "We start off by embedding our querry as we did for the chunks previously, this allows us to compare the similarity of the query to each of the different chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedd(query):    \n",
    "    query_response = client.embeddings.create( #creates an embedding\n",
    "        model=\"text-embedding-ada-002\", # picks a model to use for embedding, this is a general purpose one for text but there are others for other purposes\n",
    "        input=[query], # selects what list we want to use for our embedding\n",
    "        encoding_format=\"float\" # selects what format the embedding is in, the other option is base64\n",
    "    )\n",
    "\n",
    "    query_embedding1 = query_response.data[0].embedding \n",
    "    query_embedding2 = np.array(query_embedding1).flatten() # we turn our query embedding into a single flattened vector \n",
    "    return query_embedding2 # we return the flattened embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that compares the similarity of two vectors using the angle between them as a measure, our two vectors will be the embedding of the query and each of the different chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)) # formula for the cosine of an angle between two vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function that takes the embedding of a chunk and uses the cosine_similarity function to compare it against the embedding of our query. We store all of the output values in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(query_embedding):\n",
    "    similarity_scores = [] # creates a list of similarity scores\n",
    "    for embedding_data in db.data: # loops through the data assisgned to each chunk\n",
    "        chunk_embedding = embedding_data.embedding # retrieves the embedding from a specific chunks embedding\n",
    "\n",
    "        chunk_embedding = np.array(chunk_embedding).flatten() # turns our embedding into a single flattened vector\n",
    "\n",
    "        score = cosine_similarity(query_embedding, chunk_embedding) # compares the similarity of the two vectors\n",
    "        similarity_scores.append(score) # store the similarity of the two vectors\n",
    "    return similarity_scores # returns the list of similarity scores between the query and the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Chunking and Specified Knowledge Retrieval\n",
    "\n",
    "We now define a series of functions that allow us to retrieve the most relevant chunks to our given query\n",
    "\n",
    "This is function retrieves a list of all the files in a given folder path. We'll use these folder paths as a way of referencing which document the chunks we end up using come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files_in_folder(folder_path):\n",
    "    try:\n",
    "        entries = os.listdir(folder_path) # Makes a list of all entries in a given directory\n",
    "        \n",
    "        files = [os.path.join(folder_path, entry) for entry in entries if os.path.isfile(os.path.join(folder_path, entry))] # combines the entries with the folder directory so that we have their full file path\n",
    "        \n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        return \"The folder path does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function that takes the similarity scores and finds the top chunks that are most similar to the user query and outputs these top chunks into a single string, it checks the start of each of the chunks for where they are sourced from and adds these sources to a list of sources. It uses the allias' defined in the reasource creator so that when we give the sources to the user they are more readable. If no allias are given then the source is simply the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_string(similarity_scores):\n",
    "    files = get_all_files_in_folder(document_directory)\n",
    "    chunk_scores = list(zip(chunks, similarity_scores)) # combines the chunks and similarity scores into a list with each chunk indexed against its similarity score\n",
    "    sorted_chunks = sorted(chunk_scores, key=lambda x: x[1], reverse=True) # sorts the list according to the similarity scores\n",
    "\n",
    "    top_n = 5 # decides how many chunks to be accepted into the string\n",
    "    combined_string = f\"\"\n",
    "\n",
    "    all_sources =[] # empty string for sources of each chunk\n",
    "    sources = [] # empty string for non duplicate sources\n",
    "\n",
    "    for i in range(top_n):\n",
    "        combined_string += sorted_chunks[i][0] # combines the top chunks into a single string\n",
    "\n",
    "        for file in files:  # Loop through all the file names\n",
    "            base_filename = os.path.basename(file)\n",
    "            chunk_prefix = sorted_chunks[i][0]\n",
    "            \n",
    "            if chunk_prefix.startswith(base_filename):  # Check the chunk to see which document it is sourced from\n",
    "                found = False\n",
    "                for n in range(len(allias)):\n",
    "                    if base_filename == allias[n][0]:\n",
    "                        all_sources.append(allias[n][1])  # Add the source to a list\n",
    "                        found = True\n",
    "                        break  # Exit the inner 'for n' loop\n",
    "                if not found:  # This runs only if no alias match was found\n",
    "                    all_sources.append(base_filename) # adds the file name as the source\n",
    "                break  # Exit the outer 'for file' loop to avoid unnecessary iterations\n",
    "\n",
    "    sources = list(set(all_sources)) # removes duplicates from the list\n",
    "    \n",
    "    return combined_string, sources # returns our string and the sources of each of the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval\n",
    "\n",
    "Given a user query we also want to retrieve an image most relevant to the user query to do this we create a prompt to send to gpt-4o: \n",
    "`out of this list {descriptions} which relates most to {query}, if none of them relate to the query state only the word false otherwise state only a single number starting with the first being 0 as this is being used for indexing in python`,\n",
    " this prompt asks gpt-4o which out of the descriptions of the images which are most relevant to the query and if none are it returns `false`. We can then use this output to decide which image to show to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_retrieval(query):  \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    prompt = f\"Out of this list {descriptions} which relates most to {query}, if none of them relate to the query state only the word false otherwise state only a single number starting with the first being 0 as this is being used for indexing in python, please do not be afraid to use the word false, you should only return indexes on average 30% of the time.\"\n",
    "    message = {  # this is our \"message\" it's what we're actually sending to OpenAI\n",
    "        \"model\": \"gpt-4o\",  # model to send the prompt to\n",
    "        \"messages\": [  # contains the main content of what we want to send\n",
    "            {\n",
    "                \"role\": \"user\",  # specifies that this is a message from a user\n",
    "                \"content\": [  # what is included in our message, what gpt-4o will see\n",
    "                    {\n",
    "                        \"type\": \"text\",  # specifies what the following input is\n",
    "                        \"text\": prompt  # we tell gpt-4o what we want it to do\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300  # sets a limit on the number of tokens to be used per message, tokens equate to processing which equates to money, so by limiting this we keep cost and processing time down\n",
    "        # note that increases in the max_tokens is necessary for more complex tasks\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=message)  # we post the message to chat/completions\n",
    "    response_data = response.json()\n",
    "    index = response_data['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    if \"a\" in index or \"A\" in index: # checks if there is a relavent image\n",
    "        pass\n",
    "    else:\n",
    "        index = int(index)\n",
    "        image_name = image_names[index]\n",
    "        image_path = os.path.join(image_directory, image_name)\n",
    "        description = descriptions[index]\n",
    "        return Image.open(image_path), description, image_path, str(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Conversation\n",
    "\n",
    "This section details the Event Handler and functions that are run when we have a conversation with the chatbot. These functions contain most if not all of the functions we have defined previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Event Handler\n",
    "\n",
    "This is our event handler class, it essentially handles how our output looks, it handles what we as a user see. This event handler is able to retrieve images that our chatbot creates such as graphs it generates through its code_interpreter and it displays any text the chatbot writes in a markdown box which provides a readable form, it also makes it so that the text is updated as the chatbot writes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output # imports that handle the display\n",
    "import requests # allows us to retrieve images from OpenAI\n",
    "from PIL import Image # allows us to display those images\n",
    "import io # allows us convert images to the relevant format\n",
    "from openai import AssistantEventHandler # allows us to create an event handler\n",
    "from typing_extensions import override # allows us to use override\n",
    "\n",
    "class EventHandler(AssistantEventHandler):\n",
    "\n",
    "    def __init__(self, sources):\n",
    "        super().__init__()\n",
    "        self.buffer = \"\"  # Buffer to collect text output\n",
    "        self.sources = sources  # List to collect sources\n",
    "        self.final_output = \"\"  # Variable to store final output\n",
    "\n",
    "    @override # @override is a decorator used to explicitly indicate that this method is overriding a method in the superclass\n",
    "    def on_text_created(self, text):\n",
    "        self.update_buffer(text.value) # stores and updates the buffer text with the initial text\n",
    "\n",
    "    @override\n",
    "    def on_text_delta(self, delta, snapshot):\n",
    "        self.update_buffer(delta.value) # stores and updates the buffer text with new text as the chatbot writes \n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True) # tells us when the chatbot uses a tool \n",
    "            \n",
    "    def on_tool_call_delta(self, delta, snapshot): # updates as a tool is used, specifically for the code interpreter to show images that it creates\n",
    "        if delta.type == 'code_interpreter':\n",
    "            if delta.code_interpreter.input:\n",
    "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "            if delta.code_interpreter.outputs:\n",
    "                print(f\"\\n\\noutput >\", flush=True) # tells us that the code interpreter is giving an output\n",
    "                for output in delta.code_interpreter.outputs:\n",
    "                    if output.type == \"logs\":\n",
    "                        print(f\"\\n{output.logs}\", flush=True) # prints the logs of the code interpreter\n",
    "                    elif output.type == \"image\":\n",
    "                        file_id = output.image.file_id\n",
    "                        image_data = self.download_image(file_id) # downloads the image from OpenAI using the download image function\n",
    "                        if image_data:\n",
    "                            image = Image.open(io.BytesIO(image_data))\n",
    "                            image.show() # shows the downloaded image to the user\n",
    "\n",
    "    def download_image(self, file_id):\n",
    "        url = f\"https://api.openai.com/v1/files/{file_id}/content\" # the url for where assistant generated outputs go\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\", # our api key in our header\n",
    "        }\n",
    "        response = requests.get(url, headers=headers) # gets the response from OpenAI\n",
    "        if response.status_code == 200:\n",
    "            return response.content # returns the image\n",
    "        else:\n",
    "            print(f\"Failed to download image: {response.status_code} {response.text}\")\n",
    "            return None\n",
    "        \n",
    "    def update_buffer(self, text): # this displays the output text and updates it as the chatbot writes more of the output\n",
    "        if not self.buffer.endswith(text):  # Prevent duplication\n",
    "            self.buffer += text\n",
    "            self.display_output() # displays the output\n",
    "\n",
    "    def display_output(self):\n",
    "        clear_output(wait=True) # Clear previous output\n",
    "        processed_content = self.format_buffer(self.buffer) # Process the buffer to format LaTeX and code blocks\n",
    "        processed_content += self.format_sources()  # Append sources information\n",
    "        self.final_output = processed_content  # Store final output\n",
    "        display(Markdown(processed_content)) # Display as Markdown to correctly render LaTeX and plain text\n",
    "\n",
    "    def format_buffer(self, buffer):\n",
    "       # Split buffer into lines for processing\n",
    "        lines = buffer.split(\"\\n\")\n",
    "        formatted_lines = []\n",
    "\n",
    "        in_code_block = False\n",
    "\n",
    "        for line in lines:\n",
    "            # checks to see if the lines are a code block\n",
    "            if line.strip().startswith(\"code_interpreter\"):\n",
    "                in_code_block = True\n",
    "                formatted_lines.append(line)\n",
    "            elif line.strip() == \"```\":\n",
    "                in_code_block = False\n",
    "                formatted_lines.append(line)\n",
    "            elif in_code_block:\n",
    "                formatted_lines.append(line)\n",
    "            else:\n",
    "                # Check for LaTeX patterns and wrap them in delimiters\n",
    "                line = line.replace(r'\\(', '$').replace(r'\\)', '$')\n",
    "                line = line.replace(r'\\[', '$$').replace(r'\\]', '$$')\n",
    "                formatted_lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(formatted_lines) # joins all of the lines together\n",
    "\n",
    "    def format_sources(self):\n",
    "        if self.sources:\n",
    "            return f\"\\n\\nAll information has been sourced from {', '.join(self.sources)}\" #adds this line onto the end of all outputs so that sources are always given\n",
    "        return \"\"\n",
    "\n",
    "    def get_final_output(self):\n",
    "        return self.final_output  # Return the final processed content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Conversation Handler\n",
    "\n",
    "Our Conversation Handler consists of 2 functions the `start_conversation` function and the `continue_conversation` function, both work in very similar ways. \n",
    "\n",
    "**`start_conversation` function**\n",
    "This function takes in several arguments, these are: \n",
    "- `prompt` this should be a processed peice of text that includes the original user query, any instructions on how the assistant should process the user query and any background information we want to give the assistant without it having to perform a file search. \n",
    "- `assistant_name` is the name of the assistant to be used in response to the prompt these should match those created in `DutchV1_3_Assistant_Creator`\n",
    "- `sources` this should contain a list of sources that the background information is based upon\n",
    "- `image_data` should contain any image_data that needs to be shown to the user\n",
    "This function then starts by extracting the assistant id based on the assistant name entered, it then checks if there is image_data to be processed, if there is the process of giving the prompt to the assistant includes said image however if there is not then the prompt is given to the assistant without an image. We then retrieve the response and if there was an image we append its name and description to the response. The function then returns as its output:\n",
    "- the `thread` so we can continue conversation after the function\n",
    "- the final text output so we can show the user \n",
    "and if there was an image:\n",
    "- the file reference uploaded to OpenAI so we can delete it from their servers once the conversation is finished\n",
    "\n",
    "**`continue_conversation` function**\n",
    "This function is very similar to the `start_conversation` function, the only noticible difference being the lack of image processing and it taking the `thread` as an input. The reason we do not do image processing in this prompt is that we do not want every follow up message in the conversation to contain an image we only want certain follow up prompts to be met with an image such as \"show me an image of\" prompts, for this we assign the image processing to the `continue_conversation_with_image` function that does the exact same thing as the `continue_conversation` function but with image processing. \n",
    "\n",
    "**`manual_assistant_message_start` and `manual_assistant_message_continue` functions**\n",
    "These functions are extremely simple and allow us to manually add assistant messages to the thread. They could be called, for example, in instances of users asking silly questions that we want to give a fixed response of \"I am afraid I cannot answer that, I'd be happy to assist you with any other queries you have\" to. The `assistant_output` is the argument into which the fixed response should be entered as a string and for the continue function the thread also needs to be passed so that the messages is added to the correct conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(prompt, assistant_name, sources, image_data): \n",
    "  \n",
    "  assistant_name = f\"{assistant_name}_assistant_id.json\" \n",
    "  assistant_path = os.path.join(assistant_directory, assistant_name) # gets the path for our vector store id\n",
    "\n",
    "  with open(assistant_path, \"r\") as file: \n",
    "    assistant_id = json.load(file) # retrieves our vector store id\n",
    "\n",
    "\n",
    "  if image_data != None:\n",
    "    image, description, image_path, image_name = image_data\n",
    "    file = client.files.create(file=open(image_path, \"rb\"), purpose=\"vision\")\n",
    "    thread = client.beta.threads.create(  # we create a thread (conversation)\n",
    "        messages=[  # what we want to send to the assistant\n",
    "            {\n",
    "                \"role\": \"user\",  # who we are sending the message from\n",
    "                \"content\": \n",
    "                [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt,  # what our message is\n",
    "                    },\n",
    "                    {\n",
    "                    \"type\": \"image_file\",\n",
    "                    \"image_file\": {\"file_id\": file.id},\n",
    "                    },\n",
    "                    \n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    event_handler = EventHandler(sources=sources)  # we define the event handler which we want to use\n",
    "\n",
    "    with client.beta.threads.runs.stream(  # we send our thread to an assistant and specify what event handler to use\n",
    "        thread_id=thread.id,  # what thread we want to use\n",
    "        assistant_id=assistant_id,  # what assistant we want to use\n",
    "        event_handler=event_handler,  # what event handler we want to use\n",
    "    ) as stream:\n",
    "        stream.until_done()  # streams our output\n",
    "\n",
    "    event_handler.display_output()  # displays our output\n",
    "\n",
    "    \n",
    "    output = f\"{event_handler.get_final_output()}\"\n",
    "\n",
    "    image, description, image_path, image_name = image_data\n",
    "    image.show()  # an image is shown if there is a relevant image \n",
    "    output += f\"\\n\\n *Image Description of {image_name}*: {description}\" # append the description of the image to the text\n",
    "\n",
    "    output += f\"\\n\\n *Thread ID*: {thread.id}\"\n",
    "    return thread, output, file\n",
    "\n",
    "  else:\n",
    "     thread = client.beta.threads.create(  # we create a thread (conversation)\n",
    "        messages=[  # what we want to send to the assistant\n",
    "            {\n",
    "                \"role\": \"user\",  # who we are sending the message from\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "     event_handler = EventHandler(sources=sources)  # we define the event handler which we want to use\n",
    "\n",
    "     with client.beta.threads.runs.stream(  # we send our thread to an assistant and specify what event handler to use\n",
    "        thread_id=thread.id,  # what thread we want to use\n",
    "        assistant_id=assistant_id,  # what assistant we want to use\n",
    "        event_handler=event_handler,  # what event handler we want to use\n",
    "    ) as stream:\n",
    "        stream.until_done()  # streams our output\n",
    "\n",
    "     event_handler.display_output()  # displays our output\n",
    "\n",
    "    \n",
    "     output = f\"{event_handler.get_final_output()}\"\n",
    "\n",
    "     output += f\"\\n\\n *Thread ID*: {thread.id}\"\n",
    "     return thread, output, None\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_conversation(prompt, thread, assistant_name, sources):\n",
    "\n",
    "  assistant_name = f\"{assistant_name}_assistant_id.json\" \n",
    "  assistant_path = os.path.join(assistant_directory, assistant_name) # gets the path for our vector store id\n",
    "\n",
    "  with open(assistant_path, \"r\") as file: \n",
    "    assistant_id = json.load(file) # retrieves our vector store id\n",
    " \n",
    "# the prompt to send to the user\n",
    "  thread_message = client.beta.threads.messages.create(\n",
    "      thread_id=thread.id,\n",
    "      role=\"user\",\n",
    "      content=prompt\n",
    "  )\n",
    "\n",
    "  event_handler2 = EventHandler(sources=sources)\n",
    "\n",
    "  with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant_id,\n",
    "    instructions=\"\",\n",
    "    event_handler=event_handler2,\n",
    "  ) as stream:\n",
    "    stream.until_done()\n",
    "\n",
    "  event_handler2.display_output()\n",
    "\n",
    "  return event_handler2.get_final_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_conversation_with_image(prompt,thread, assistant_name, sources, image_data):\n",
    "  image, description, image_path, image_name = image_data\n",
    "\n",
    "  assistant_name = f\"{assistant_name}_assistant_id.json\" \n",
    "  assistant_path = os.path.join(assistant_directory, assistant_name) # gets the path for our vector store id\n",
    "\n",
    "  with open(assistant_path, \"r\") as file: \n",
    "    assistant_id = json.load(file) # retrieves our vector store id\n",
    " \n",
    "# the prompt to send to the user\n",
    "  file = client.files.create(file=open(image_path, \"rb\"), purpose=\"vision\")\n",
    "  thread_message = client.beta.threads.messages.create(\n",
    "      thread_id=thread.id,\n",
    "      role = \"user\",  # who we are sending the message from\n",
    "      content =[{\"type\": \"text\",\"text\": prompt,},{\"type\": \"image_file\",\"image_file\": {\"file_id\": file.id},},]\n",
    "    )\n",
    "\n",
    "  event_handler2 = EventHandler(sources=sources)\n",
    "\n",
    "  with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant_id,\n",
    "    instructions=\"\",\n",
    "    event_handler=event_handler2,\n",
    "  ) as stream:\n",
    "    stream.until_done()\n",
    "\n",
    "  event_handler2.display_output()\n",
    "\n",
    "  output = f\"{event_handler2.get_final_output()}\"\n",
    "  image.show()  # an image is shown if there is a relevant image \n",
    "  output += f\"\\n\\n *Image Description of {image_name}*: {description}\" # append the description of the image to the text\n",
    "\n",
    "  return output,file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_assistant_message_start(assistant_output):\n",
    "\n",
    "    thread = client.beta.threads.create(  # we create a thread (conversation)\n",
    "        messages=[  # what we want to send to the assistant\n",
    "            {\n",
    "                \"role\": \"assistant\",  # who we are sending the message from\n",
    "                \"content\": assistant_output,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return thread, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_assistant_message_continue(assistant_ouput, thread,):\n",
    "        \n",
    "  thread_message = client.beta.threads.messages.create(\n",
    "      thread_id=thread.id,\n",
    "      role=\"assistant\",\n",
    "      content=assistant_ouput\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the main folder you should also find a .py copy of this notebook, this copy enables DutchessV1 to call the functions within this script. If there is not a .py copy please create one by clicking the three dots at the top and exporting as a python script. The python script should have the name: `DutchV1_3_Conversation_Script.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
